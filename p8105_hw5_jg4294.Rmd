---
title: "P8105 HW5"
author: JingYao Geng
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

```{r setup, include = FALSE, warning=FALSE}
library(tidyverse)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

**Read in the homicide dataset.**
```{r import data}
homicide_df = 
  read_csv("data1/homicide-data.csv")  
```

The original homicide data contains `r nrow(homicide_df)` observations and `r ncol(homicide_df)` variables. The variables are including information about the location (latitude and longitude) of the homicide and the victim (first and last name, age, sex, race).  

**Create variable `city_state`**
```{r city_state}
homicide_df =
  homicide_df %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL")
```

**Summarize within cities to obtain the total number of homicides and the number of unsolved homicides**

```{r}
aggregate_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```

**For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.**

```{r}
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved), 
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>% 
  broom::tidy()
```

**Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.**

```{r}
results_df = 
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```

**Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides**

```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

**Alternative way with function**
```{r, error = TRUE, eval=FALSE}
city_prop_test = function(df) {
  
  n_unsovled ...
  n_total ... 
  
  prop.test(.....)
  
}
homicide_df = 
  read_csv("data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL") %>% 
  nest(data = resolved)
```

## Problem 2

A longitudinal study that included a control arm and an experimental arm. Data for each participant is included in a separate file, and file names include the subject ID and arm.

**Read in the data and tidy the dataframe containing data from all participants, including the subject ID, arm, and observations over time** 

```{r 2import}
files = list.files("./lda_data", pattern = ".csv", all.files = FALSE, 
full.names = FALSE)

longit_study = data.frame(participants = files) %>% 
  janitor::clean_names() %>%
  mutate(
    file_contents = 
      map(participants, ~read_csv(file.path("./lda_data", .)))
    ) %>% 
  separate(participants, into = c("control", "subject_id")) %>% 
  unnest(file_contents) %>% 
  mutate(
    control = recode(control, `con` = "control", `exp` = "experiment")
  ) 

# Tidy
longit_study_tidy = longit_study %>% 
  pivot_longer(week_1:week_8,
               names_to = "week",
               names_prefix = "week_",
               values_to = "observation") 
```

**Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.**

```{r}
longit_study_tidy %>%
  ggplot(aes(x = week, y = observation, group = subject_id, color = subject_id)) +
    geom_path() + 
    facet_grid(~control) +
    labs(
      title = "Weekly Observations for Control and Experiment Groups",
      x = "Week",
      y = "Observation"
  )
```

**Comments: there is a clear increase pattern in the experiment group over the time for 8 weeks. The patterns of the control group are hard to tell, and the overall trends tend to be flatter comparing to the experiment group.**

## Problem 3

```{r}
set.seed(1)

sim = function(n=30, mu, sigma=5) {
  
x = rnorm(30, mean = mu, 5)
mu_hat = mean(x)
test_result = t.test(x) %>% broom::tidy()
p_value = test_result$p.value

tibble(mu_hat, p_value)
  
}

output = vector("list", 5000)

for (i in 1:5000) {
  output[[i]] = sim(mu = 0)
}

results = bind_rows(output)
results

```

