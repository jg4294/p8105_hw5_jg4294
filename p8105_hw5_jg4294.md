P8105 HW5
================
JingYao Geng
2020-11-18

## Problem 1

**Read in the homicide dataset.**

``` r
homicide_df = 
  read_csv("data1/homicide-data.csv")  
```

    ## Parsed with column specification:
    ## cols(
    ##   uid = col_character(),
    ##   reported_date = col_double(),
    ##   victim_last = col_character(),
    ##   victim_first = col_character(),
    ##   victim_race = col_character(),
    ##   victim_age = col_character(),
    ##   victim_sex = col_character(),
    ##   city = col_character(),
    ##   state = col_character(),
    ##   lat = col_double(),
    ##   lon = col_double(),
    ##   disposition = col_character()
    ## )

The original homicide data contains 52179 observations and 12 variables.
The variables are including information about the location (latitude and
longitude) of the homicide and the victim (first and last name, age,
sex, race).

**Create variable `city_state`**

``` r
homicide_df =
  homicide_df %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL")
```

**Summarize within cities to obtain the total number of homicides and
the number of unsolved homicides**

``` r
aggregate_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

**For the city of Baltimore, MD, use the prop.test function to estimate
the proportion of homicides that are unsolved; save the output of
prop.test as an R object, apply the broom::tidy to this object and pull
the estimated proportion and confidence intervals from the resulting
tidy dataframe.**

``` r
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved), 
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>% 
  broom::tidy()
```

    ## # A tibble: 1 x 8
    ##   estimate statistic  p.value parameter conf.low conf.high method    alternative
    ##      <dbl>     <dbl>    <dbl>     <int>    <dbl>     <dbl> <chr>     <chr>      
    ## 1    0.646      239. 6.46e-54         1    0.628     0.663 1-sample~ two.sided

**Now run prop.test for each of the cities in your dataset, and extract
both the proportion of unsolved homicides and the confidence interval
for each. Do this within a “tidy” pipeline, making use of purrr::map,
purrr::map2, list columns and unnest as necessary to create a tidy
dataframe with estimated proportions and CIs for each city.**

``` r
results_df = 
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```

**Create a plot that shows the estimates and CIs for each city – check
out geom\_errorbar for a way to add error bars based on the upper and
lower limits. Organize cities according to the proportion of unsolved
homicides**

``` r
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

<img src="p8105_hw5_jg4294_files/figure-gfm/unnamed-chunk-4-1.png" width="90%" />

**Alternative way with function**

``` r
city_prop_test = function(df) {
  
  n_unsovled ...
  n_total ... 
  
  prop.test(.....)
  
}
homicide_df = 
  read_csv("data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL") %>% 
  nest(data = resolved)
```

## Problem 2

A longitudinal study that included a control arm and an experimental
arm. Data for each participant is included in a separate file, and file
names include the subject ID and arm.

**Read in the data and tidy the dataframe containing data from all
participants, including the subject ID, arm, and observations over
time**

``` r
files = list.files("./lda_data", pattern = ".csv", all.files = FALSE, 
full.names = FALSE)

longit_study = data.frame(participants = files) %>% 
  janitor::clean_names() %>%
  mutate(
    file_contents = 
      map(participants, ~read_csv(file.path("./lda_data", .)))
    ) %>% 
  separate(participants, into = c("control", "subject_id")) %>% 
  unnest(file_contents) %>% 
  mutate(
    control = recode(control, `con` = "control", `exp` = "experiment")
  ) 
```

    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )

    ## Warning: Expected 2 pieces. Additional pieces discarded in 20 rows [1, 2, 3, 4,
    ## 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].

``` r
# Tidy
longit_study_tidy = longit_study %>% 
  pivot_longer(week_1:week_8,
               names_to = "week",
               names_prefix = "week_",
               values_to = "observation") 
longit_study_tidy 
```

    ## # A tibble: 160 x 4
    ##    control subject_id week  observation
    ##    <chr>   <chr>      <chr>       <dbl>
    ##  1 control 01         1            0.2 
    ##  2 control 01         2           -1.31
    ##  3 control 01         3            0.66
    ##  4 control 01         4            1.96
    ##  5 control 01         5            0.23
    ##  6 control 01         6            1.09
    ##  7 control 01         7            0.05
    ##  8 control 01         8            1.94
    ##  9 control 02         1            1.13
    ## 10 control 02         2           -0.88
    ## # ... with 150 more rows

**Make a spaghetti plot showing observations on each subject over time,
and comment on differences between groups.**

``` r
longit_study_tidy %>%
  ggplot(aes(x = week, y = observation, group = subject_id, color = subject_id)) +
    geom_path() + 
    facet_grid(~control) +
    labs(
      title = "Weekly Observations for Control and Experiment Groups",
      x = "Week",
      y = "Observation"
  )
```

<img src="p8105_hw5_jg4294_files/figure-gfm/unnamed-chunk-6-1.png" width="90%" />

**Comments: there is a clear increase pattern in the experiment group
over the time for 8 weeks. The patterns of the control group are hard to
tell, and the overall trends tend to be flatter comparing to the
experiment group.**

## Problem 3

``` r
set.seed(1)
sim = function(n=30, mu, sigma=5) {
  
x = rnorm(30, mean = mu, 5)
mu_hat = mean(x)
test_result = t.test(x) %>% broom::tidy()
p_value = test_result$p.value
tibble(mu_hat, p_value)
  
}


### When μ=0, generate 5000 datasets
output = vector("list", 5000)
for (i in 1:5000) {
  output[[i]] = sim(mu = 0)
}
results = bind_rows(output)
head(results)
```

    ## # A tibble: 6 x 2
    ##   mu_hat p_value
    ##    <dbl>   <dbl>
    ## 1  0.412  0.629 
    ## 2  0.664  0.368 
    ## 3  0.551  0.534 
    ## 4  0.567  0.487 
    ## 5 -1.65   0.0599
    ## 6  1.19   0.229

**Use list columns and map for** \(\mu= 0,1,2,3,4,5,6\)

``` r
sim_results = 
  tibble(mu = c(0, 1, 2, 3, 4, 5, 6)) %>% 
  mutate(
    output_lists = map(.x = mu, ~rerun(5000, sim(mu = .x))),
    estimate_dfs = map(output_lists, bind_rows)) %>% 
  select(-output_lists) %>% 
  unnest(estimate_dfs)

head(sim_results)
```

    ## # A tibble: 6 x 3
    ##      mu mu_hat p_value
    ##   <dbl>  <dbl>   <dbl>
    ## 1     0  0.520   0.549
    ## 2     0  1.11    0.188
    ## 3     0  1.01    0.220
    ## 4     0 -0.835   0.309
    ## 5     0 -0.892   0.365
    ## 6     0  0.557   0.503

**Plot the proportion of times the null was rejected (the power of the
test) on the y axis and the true value of μ on the x axis.**

``` r
sim_results %>% 
  group_by(mu) %>% 
  filter(p_value < 0.05) %>% 
  summarize(
    rej = n()
  ) %>% 
  mutate(proportion_of_rej = rej / 5000) %>% 
  ggplot(aes(x = mu, y = proportion_of_rej, color = mu)) +
  geom_point() + 
  geom_line() +
  ylim(0,1) +
  geom_text(aes(label = proportion_of_rej, hjust = 0, vjust = 0.5)) +
  labs(
    title = "Power of the test",
    x = "True mu",
    y = "Power"
  )
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

<img src="p8105_hw5_jg4294_files/figure-gfm/proportion_of_rej-1.png" width="90%" />

From the proportion of reject plot we can see that the larger the effect
size, the larger the power. Hence, as the effect size increases, we have
greater probability to reject the false null hypothesis. When the true
mu equals to 5 or 6, the power of the test reaches to 1.

**Make a plot showing the average estimate of μ\_hat on the y axis and
the true value of μ on the x axis. Make a second plot (or overlay on the
first) the average estimate of μ^ only in samples for which the null was
rejected on the y axis and the true value of μ on the x axis.**

``` r
plot1 =
  sim_results %>% 
  group_by(mu) %>% 
  summarize(
    average_mu_hat = mean(mu_hat)
  ) %>% 
  ggplot(aes(x = mu, y = average_mu_hat, color = mu)) +
  geom_point() + geom_line() + 
  labs(
    title = "Average estimate of mu_hat vs. True mu ",
    x = "True mu",
    y = "Average estimate mu hat"
  )
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

``` r
plot1
```

<img src="p8105_hw5_jg4294_files/figure-gfm/plot1-1.png" width="90%" />

``` r
plot2 = 
  sim_results %>% 
  group_by(mu) %>% 
  filter(p_value < 0.05) %>% 
  summarize(
    average_rej_mu = mean(mu_hat)
  ) %>% 
  ggplot(aes(x = mu, y = average_rej_mu, color = mu)) +
  geom_point() + geom_line() +
  labs(
    title = "Rejected: Average estimate of mu_hat vs. True mu",
    x = "True mu",
    y = "Average esitmate mu (rejected)"
  )
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

``` r
plot2
```

<img src="p8105_hw5_jg4294_files/figure-gfm/plot2-1.png" width="90%" />

Based on plot1, we find that the average estimated mu\_hats from our
samples are the same as the true mu (from 0 to 6). And based on plot2,
we find that the sample average of mu\_hat across tests for which the
null is rejected approximately equal to the true value of mu when mu is
equal to 4, 5, and 6. However, for mu = 0,1,2,3, the sample average of
mu for which the null is rejected deviates significantly from the true
value of mu. This is reasonable because as the effect size (in other
words the true mu) increases, the power of the test increases as well.
We would expect to see a increase in the probability that a false null
hypothesis is rejected when the true mu increases .
